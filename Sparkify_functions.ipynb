{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.sql.types import IntegerType, ArrayType, FloatType, DoubleType, Row, DateType\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import  MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell is used on AWS EMR\n",
    "\n",
    "# import os\n",
    "# import re\n",
    "# import sys\n",
    "# print(\"start SPARK application\")\n",
    "# os.environ[\"SPARK_HOME\"] = \"/usr/lib/spark/\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = \"/mnt/anaconda3/bin/python3.7\"\n",
    "# spark_home = os.environ.get('SPARK_HOME', None)\n",
    "# sys.path.insert(0, spark_home + \"/python\")\n",
    "# sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell is used on AWS EMR\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.master(\"yarn\") \\\n",
    "#     .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "#     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "#     .config(\"spark.shuffle.spill.compress\", \"true\") \\\n",
    "#     .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
    "#     .config(\"spark.io.compression.codec\", \"snappy\") \\\n",
    "#     .config(\"spark.driver.memory\", \"24g\") \\\n",
    "#     .config(\"spark.driver.cores\", \"4\") \\\n",
    "#     .config(\"spark.executor.cores\", \"4\") \\\n",
    "#     .config(\"spark.executor.memory\", \"24g\") \\\n",
    "#     .config(\"spark.kryoserializer.buffer.max\", \"2000m\") \\\n",
    "#     .config(\"spark.network.timeout\", \"360000\") \\\n",
    "#     .config(\"spark.dynamicAllocation.minExecutors\", \"25\")\\\n",
    "#     .enableHiveSupport() \\\n",
    "#     .getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.broadcastTimeout\", 72000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.json('s3://<bucket-name>/ElifSurmeli/' + event_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pd.DataFrame(df.take(5), columns=df.columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zanin\\AppData\\Local\\Temp\\ipykernel_18440\\3408833990.py:3: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Thales-PC:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sparkify</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x256eb1a84f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder.master('local[*]').appName(\"Sparkify\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = './mini_sparkify_event_data.json'\n",
    "\n",
    "df = spark.read.json(path)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(df):\n",
    "    \"\"\"\n",
    "    Casts miliseconds ts column and creates two new columns with timestamptype and datetype,\n",
    "    which will be used for further processing\n",
    "    Extracts days from ts\n",
    "    Casts user id to LongType\n",
    "    Splits location field and takes only state name as location.\n",
    "        -   parameters: df (spark DataFrame)\n",
    "        -   returns: df preprocessed Spark DataFrame\n",
    "    \"\"\"\n",
    "    # Changing userId column to Longtype and removing null from userId\n",
    "    df = df.withColumn('userId', f.col('userId').cast(LongType()))\n",
    "    df = df.filter(f.col('userId').isNotNull())\n",
    "    \n",
    "    # Removing LoggedOut from column auth.\n",
    "    df = df.filter(f.col('auth')!='LoggedOut')\n",
    "\n",
    "    # taking only the state of location\n",
    "    df = df.withColumn('location', f.split(f.col('location'),',').getItem(1))\n",
    "\n",
    "    # Convert the timestamps in columns 'registration' and 'ts' into datetime format and add them to the dataframe\n",
    "    df = df.withColumn('registrationDate', (df.registration / 1000).cast(TimestampType()))\n",
    "    df = df.withColumn('tsDate', (df.ts / 1000).cast(TimestampType()))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def churns_data(df):\n",
    "    \"\"\"\n",
    "    Creating label column based on churn of user.\n",
    "    -   parameters: df (spark DataFrame)\n",
    "    -   returns: df with calculated column\n",
    "    \"\"\"\n",
    "    # defining Churn data\n",
    "    label_df = df.withColumn('label', when((col('page').isin(['Cancellation Confirmation','Cancel'])) | (col('auth')=='Cancelled'),1 ).otherwise(0))\\\n",
    "        .groupBy('userId').agg(sum('label').alias('label')).withColumn('label', when(col('label')>=1 ,1).otherwise(0))\n",
    "\n",
    "    df = df.join(label_df, on='userId')\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_on_platform(df):\n",
    "    \"\"\"\n",
    "    Calculate the time that user in registered on the platform based on registration date and last interaction\n",
    "    -   parameters: df (spark DataFrame)\n",
    "    -   returns: df with who many days the user has been registered on the platform.\n",
    "    \"\"\"\n",
    "    \n",
    "    #crating a column of last interaction per user\n",
    "    last_interaction_df =  df.groupBy('userId').agg(max('ts').alias('lastIteraction'))\n",
    "    df = last_interaction_df.join(df, on= 'userId', how='left').withColumn('registeredDays', ((f.col('lastIteraction')-f.col('registration'))/86400000).cast(IntegerType()))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_level(df):\n",
    "    \"\"\"\n",
    "    Search the latest level of each user is was paid or free tier.\n",
    "    -   parameters: df (spark DataFrame)\n",
    "    -   returns: df with latest level assigned to each user\n",
    "    \"\"\"\n",
    "    level_df = df.orderBy('ts', ascending=False).groupBy('userId').agg(first('level').alias('last_level'))\n",
    "    \n",
    "    df = df.drop('level')\n",
    "    df = df.join(level_df, on='userId')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_length(df):\n",
    "    \"\"\"\n",
    "    Finds the averages for each length per users by day.\n",
    "    -  parameters: df (spark DataFrame)\n",
    "    -  returns: df with average length column DataFrame\n",
    "    \"\"\"\n",
    "    avg_length_df = df.groupBy('userId').avg('length').withColumnRenamed('avg(length)', 'length')\n",
    "\n",
    "    df = df.drop('length')\n",
    "    df = df.join(avg_length_df, on='userId')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_sessionid(df):\n",
    "    \"\"\"\n",
    "    Calculates daily average of distinct sessionId for each user\n",
    "    -  parameters: df (spark DataFrame)\n",
    "    -  returns: daily and monthly aggregates DataFrame\n",
    "    \"\"\"\n",
    "    daily_session_df = df.groupby('userId','tsDate').agg(countDistinct('sessionId')).\\\n",
    "        groupBy('userId').avg('count(sessionId)').\\\n",
    "        withColumnRenamed('avg(count(sessionId))', 'avg_daily_sessions')\n",
    "\n",
    "    return daily_session_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_items(df):\n",
    "    \"\"\"\n",
    "    Finds the averages for each item per users by day. \n",
    "    -  parameters: df (spark DataFrame)\n",
    "    -  returns: df with daily averages DataFrame\n",
    "    \"\"\"\n",
    "    items_per_day_df = df.groupby('userId','tsDate').agg(max('itemInSession')).\\\n",
    "    groupBy('userId').avg('max(itemInSession)').\\\n",
    "    withColumnRenamed('avg(max(itemInSession))', 'avg_daily_items')\n",
    "\n",
    "    return items_per_day_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_session(df):\n",
    "    \"\"\"\n",
    "    Finds the averages for each item per users by day.\n",
    "    -  parameters: df (spark DataFrame)\n",
    "    -  returns: df with daily duration session DataFrame\n",
    "    \"\"\"\n",
    "    #Calculates daily average of distinct sessionId for each user\n",
    "    duration_session_per_day_df = df.groupby('userId','tsDate','sessionId').agg(max('ts').alias('session_end'), min('ts').alias('session_start')).withColumn('session_duration_sec', (col('session_end')-col('session_start'))*0.001).\\\n",
    "        groupby('userId','tsDate').avg('session_duration_sec').groupby('userId').agg(mean('avg(session_duration_sec)').alias('avg_daily_session_duration')).orderBy('userId', ascending=False)\n",
    "    \n",
    "    return duration_session_per_day_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pages_aggregation(df):\n",
    "    \"\"\"\n",
    "    Finds the averages for each page event per users by day \n",
    "    -  parameters: df (spark DataFrame)\n",
    "    -  returns: df with daily duration session DataFrame\n",
    "    \"\"\"\n",
    "    unique_pages = [row.page for row in df.select('page').distinct().collect()]\n",
    "    unique_pages.remove('Cancel')\n",
    "    unique_pages.remove('Cancellation Confirmation')\n",
    "        \n",
    "    page_event_per_day_df = df.groupby('userId','tsDate').pivot('page').count()\n",
    "    exp_dict={}\n",
    "    for page in unique_pages:\n",
    "        exp_dict.update({page:'mean'})\n",
    "\n",
    "    page_event_per_day_df = page_event_per_day_df.join(page_event_per_day_df.groupBy('userId').agg(exp_dict).fillna(0), on='userId')\n",
    "\n",
    "    for page in unique_pages:\n",
    "        page_event_per_day_df = page_event_per_day_df.drop(page)  \n",
    "        page_event_per_day_df = page_event_per_day_df.withColumnRenamed('avg({})'.format(page), 'avg_daily_{}'.format(page))\n",
    "\n",
    "    page_event_per_day_df = page_event_per_day_df.drop('Cancel','Cancellation Confirmation','tsDate').drop_duplicates()\n",
    "\n",
    "    return page_event_per_day_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_features(df, session_per_user_df, duration_session_per_day_df, items_per_day_df, page_event_per_day_df):\n",
    "    \"\"\"\n",
    "    #Joins all feature engineering dataframes and main dataframe for couple of original columns\n",
    "    #artist, song, method, status, userAgent have been removed to avoid complexity\n",
    "    #firstName, lastName has been removed as they're redundant.user id sufficient to identify customer\n",
    "    -   parameters:  main DF, df_seesion_per_user, df_duration_per_day, df_items per_day, df_page_event_per_day\n",
    "    -   returns: df with all features ready to be processed\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    all_aggs_df =\\\n",
    "        session_per_user_df\\\n",
    "        .join(duration_session_per_day_df, on= 'userId')\\\n",
    "        .join(items_per_day_df, on='userId')\\\n",
    "        .join(page_event_per_day_df, on='userId')\n",
    "\n",
    "    df = df.drop('auth', 'level','userAgent','tsDate','interaction_time','registration', 'ts','song','page','itemInSession','sessionId','artist','firstName','lastName','method','status')\n",
    "    joined_df = all_aggs_df.join(df, on='userId')\n",
    "        \n",
    "    joined_df = joined_df.drop_duplicates()\n",
    "    df_features = joined_df.drop('userId', 'tsDate', 'registrationDate')\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(num_cols):\n",
    "    \"\"\"\n",
    "    Process all categorical and text columns with string indexer. \n",
    "    Process all numerical columns with Vector assembler.\n",
    "    Creates a pipeline with indexer and assembler to process all columns at once.\n",
    "    -   parameters: list of numerical column names\n",
    "    -   returns: data processed by the pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    indexer_gender = StringIndexer(inputCol='gender', outputCol='gender_index')\n",
    "    indexer_location = StringIndexer(inputCol='location', outputCol='location_index')\n",
    "    indexer_last_level = StringIndexer(inputCol='last_level', outputCol='last_level_index')\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=num_cols, outputCol='features')\n",
    "\n",
    "    process_pipeline = Pipeline(stages=[indexer_gender, indexer_location, indexer_last_level, assembler])\n",
    "\n",
    "    return process_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_data(df_features):\n",
    "    \"\"\"\n",
    "    Transforms features with feature pipeline\n",
    "    :param features_df: Spark DataFrame of features\n",
    "    :return Spark Dataframe with label and assembled features\n",
    "    \"\"\"\n",
    "    num_cols = []\n",
    "    \n",
    "    for field in df_features.schema.fields :\n",
    "        if field.dataType!=StringType():\n",
    "            num_cols.append(field.name)\n",
    "\n",
    "    num_cols.remove('label')\n",
    "\n",
    "    process_pipeline = build_pipeline(num_cols)\n",
    "    model_df = process_pipeline.fit(df_features).transform(df_features)\n",
    "    \n",
    "    return model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(train, test, model):\n",
    "    \"\"\"\n",
    "    This function fits training and runs predictions with given training and test datasets and displays results for given model\n",
    "    -   parameters train: Spark dataframe with training data\n",
    "    -   parameters test:  Spark data frame with testing data\n",
    "    -   parameters model : model name as string, either 'logistic_regression', 'random_forest' or 'gradient_boosting'\n",
    "    -   returns: A classification model trained, and the results. \n",
    "    \"\"\"\n",
    "\n",
    "    if model == 'logistic_regression':\n",
    "        ml = LogisticRegression()\n",
    "    elif model == 'random_forest':\n",
    "        ml = RandomForestClassifier()\n",
    "    elif model == 'gradient_boosting':\n",
    "        ml = GBTClassifier()\n",
    "    else:\n",
    "        return \"Please choose an appropriate model\"\n",
    "    \n",
    "    # Fit and calculate predictions\n",
    "    clf = ml.fit(train)\n",
    "    results = clf.transform(test)\n",
    "\n",
    "    print(compare_models({model : results}))\n",
    "\n",
    "    return clf, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(list_prediction):\n",
    "  \"\"\"\n",
    "  This will compare the results of each model based in: confusion matrix, accuracy, precision, recall, f1-score and AUC\n",
    "  -  parameters: list of results of each model\n",
    "  -  returns: Confusion Matrix, Accuracy, Precision, Recall, F1 and AUC.\n",
    "  \"\"\"\n",
    "  # ‘s’ would be the returned string will collect and build the confusion matrix and also the metrics (accuracy, precision, recall and F1-score)\n",
    "  \n",
    "  s = '\\n'\n",
    "\n",
    "  for model, df_transform_model in list_prediction.items():\n",
    "\n",
    "    s += '=' * 50 + '\\n' \n",
    "    s += model + ':\\n'\n",
    "    s += '-' * 50 + '\\n'\n",
    "    s += ' \\n'\n",
    "\n",
    "    # Creating the Confusion Matrix: tp = true positive, tn = true negative, fp = false positive, fn = false negative\n",
    "    tp = df_transform_model.select('label', 'prediction').where((f.col('label') == 1) & (f.col('prediction') == 1)).count()\n",
    "    tn = df_transform_model.select('label', 'prediction').where((f.col('label') == 0) & (f.col('prediction') == 0)).count()\n",
    "    fp = df_transform_model.select('label', 'prediction').where((f.col('label') == 0) & (f.col('prediction') == 1)).count()\n",
    "    fn = df_transform_model.select('label', 'prediction').where((f.col('label') == 1) & (f.col('prediction') == 0)).count()\n",
    "\n",
    "    # Ploting the Confusion Matrix.  \n",
    "    s += ' '*25 + 'Predict:\\n'\n",
    "    s += ' '*18 +  'Churn:' + ' '*5 + 'Not-Churn:\\n'\n",
    "    s += ' '*8 + 'Churn:' + ' '*7 +  str(int(tp)) + ' '*11 + str(int(fn)) + '\\n'\n",
    "    s += 'Real:\\n'\n",
    "    s += ' '*8 + 'Not-Churn:' + ' '*3 + str(int(fp)) +  ' '*11 + str(int(tn))  + '\\n'\n",
    "    s += '\\n'\n",
    "\n",
    "    # Evaluation metric using: MulticlassClassificationEvaluator and BinaryClassificationEvaluator \n",
    "    evaluator = MulticlassClassificationEvaluator()\n",
    "    auc_evaluator = BinaryClassificationEvaluator()\n",
    "    f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "    f1_score = f1_score_evaluator.evaluate(df_transform_model.select(col('label'), col('prediction')))\n",
    "\n",
    "    s += f'Accuracy: {evaluator.evaluate(df_transform_model, {evaluator.metricName: \"accuracy\"})*100:.2f}%\\n'\n",
    "    s += f'Precision: {evaluator.evaluate(df_transform_model, {evaluator.metricName: \"precisionByLabel\", evaluator.metricLabel: 1})*100:.2f}%\\n'\n",
    "    s += f'Recall: {evaluator.evaluate(df_transform_model, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 1})*100:.2f}%\\n'\n",
    "    s += f'F1: {f1_score:.2%}%\\n'\n",
    "    s += f'AUC: {auc_evaluator.evaluate(df_transform_model, {auc_evaluator.metricName: \"areaUnderROC\"})*100:.2f}%\\n' \n",
    "\n",
    "\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_model(results):\n",
    "    \"\"\"\n",
    "    This function evaluate the model trained with 2 metrics f1 and AUC\n",
    "    -   parameters results: Spark DataFrame, model prediction outputs\n",
    "    -   returns: Print the metrics f1 and AUC (areaUnderROC).\n",
    "    \"\"\"\n",
    "\n",
    "    f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "    f1_score = f1_score_evaluator.evaluate(results.select(col('label'), col('prediction')))\n",
    "    print('The F1 score on the test set is {:.2%}'.format(f1_score)) \n",
    "    print('')\n",
    "    auc_evaluator = BinaryClassificationEvaluator()\n",
    "    metric_value = auc_evaluator.evaluate(results, {auc_evaluator.metricName: \"areaUnderROC\"})\n",
    "    print('The areaUnderROC on the test set is {:.2%}'.format(metric_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuned_randomforest(train, test, numTrees=[75, 100], maxDepth=[10,20]):\n",
    "    \"\"\"\n",
    "    This function tunes the random forest, searching for the best parameters for the model using gridsearch to find numTrees and maxDepth, evaluates the results and prints best parameters\n",
    "    -   parameter train: training data\n",
    "    -   parameter test: test data\n",
    "    -   parameter numTrees: List of integers,Number of trees in the random forest\n",
    "    -   param maxDepth: List of integers, Maximum depth of the tree\n",
    "    -   returns: Prints the best features based on importance.\n",
    "    \"\"\"\n",
    "    clf = RandomForestClassifier()\n",
    "    \n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(clf.numTrees, numTrees) \\\n",
    "        .addGrid(clf.maxDepth, maxDepth) \\\n",
    "        .build()   \n",
    "    \n",
    "    crossval = CrossValidator(estimator = Pipeline(stages=[clf]),\n",
    "                         estimatorParamMaps = paramGrid,\n",
    "                         evaluator = MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                         numFolds = 3)\n",
    "\n",
    "    cvModel = crossval.fit(train)\n",
    "    predictions = cvModel.transform(test)\n",
    "    \n",
    "    evaluation_model(predictions)\n",
    "    \n",
    "    bestPipeline = cvModel.bestModel\n",
    "\n",
    "    print('======FEATURES=========IMPORTANCE=========')\n",
    "    # prints feature importances\n",
    "    for i in range(len(bestPipeline.stages[0].featureImportances)):\n",
    "        print(f\"{features_df.columns[i]} : {bestPipeline.stages[0].featureImportances[i]} \\n\")\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleaning_data(df)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = churns_data(df)\n",
    "df = days_on_platform(df)\n",
    "df = last_level(df)\n",
    "df = mean_length(df)\n",
    "session_per_user_df = agg_sessionid(df)\n",
    "items_per_day_df = mean_items(df)\n",
    "duration_session_per_day_df = duration_session(df)\n",
    "page_event_per_day_df = pages_aggregation(df)\n",
    "\n",
    "features_df = join_features(df, session_per_user_df, items_per_day_df, duration_session_per_day_df, page_event_per_day_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------------+------------------+--------------------+---------------------+-------------------+------------------------+---------------------+------------------+---------------+---------------+-----------------+-------------------------+--------------+-------------------+----------------+--------------------------+-----------------------+--------------+--------------+------+--------+-----+--------------+----------+------------------+\n",
      "|avg_daily_sessions|   avg_daily_items|avg_daily_session_duration|avg_daily_Settings|avg_daily_Add Friend|avg_daily_Thumbs Down|avg_daily_Downgrade|avg_daily_Submit Upgrade|avg_daily_Roll Advert|avg_daily_NextSong|avg_daily_Error|avg_daily_About|avg_daily_Upgrade|avg_daily_Add to Playlist|avg_daily_Home|avg_daily_Thumbs Up|avg_daily_Logout|avg_daily_Submit Downgrade|avg_daily_Save Settings|avg_daily_Help|lastIteraction|gender|location|label|registeredDays|last_level|            length|\n",
      "+------------------+------------------+--------------------------+------------------+--------------------+---------------------+-------------------+------------------------+---------------------+------------------+---------------+---------------+-----------------+-------------------------+--------------+-------------------+----------------+--------------------------+-----------------------+--------------+--------------+------+--------+-----+--------------+----------+------------------+\n",
      "|               1.0| 27.82587064676617|                       0.0|               1.0|                 1.0|                  1.0|                0.0|                     0.0|                  1.0|               1.0|            1.0|            0.0|              1.0|                      1.0|           1.0|                1.0|             1.0|                       0.0|                    1.0|           1.0| 1542955611000|     M|      WA|    0|            72|      free|253.56058066666665|\n",
      "|               1.0|137.75875486381324|                       0.0|               1.0|                 1.0|                  1.0|                1.0|                     0.0|                  0.0|               1.0|            0.0|            1.0|              0.0|                      1.0|           1.0|                1.0|             1.0|                       0.0|                    0.0|           0.0| 1539159711000|     F|      OH|    0|            21|      paid|252.22654569444447|\n",
      "+------------------+------------------+--------------------------+------------------+--------------------+---------------------+-------------------+------------------------+---------------------+------------------+---------------+---------------+-----------------+-------------------------+--------------+-------------------+----------------+--------------------------+-----------------------+--------------+--------------+------+--------+-----+--------------+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_daily_sessions</th>\n",
       "      <th>avg_daily_items</th>\n",
       "      <th>avg_daily_session_duration</th>\n",
       "      <th>avg_daily_Settings</th>\n",
       "      <th>avg_daily_Add Friend</th>\n",
       "      <th>avg_daily_Thumbs Down</th>\n",
       "      <th>avg_daily_Downgrade</th>\n",
       "      <th>avg_daily_Submit Upgrade</th>\n",
       "      <th>avg_daily_Roll Advert</th>\n",
       "      <th>avg_daily_NextSong</th>\n",
       "      <th>avg_daily_Error</th>\n",
       "      <th>avg_daily_About</th>\n",
       "      <th>avg_daily_Upgrade</th>\n",
       "      <th>avg_daily_Add to Playlist</th>\n",
       "      <th>avg_daily_Home</th>\n",
       "      <th>avg_daily_Thumbs Up</th>\n",
       "      <th>avg_daily_Logout</th>\n",
       "      <th>avg_daily_Submit Downgrade</th>\n",
       "      <th>avg_daily_Save Settings</th>\n",
       "      <th>avg_daily_Help</th>\n",
       "      <th>lastIteraction</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>label</th>\n",
       "      <th>registeredDays</th>\n",
       "      <th>last_level</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27.825871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1542955611000</td>\n",
       "      <td>M</td>\n",
       "      <td>WA</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>free</td>\n",
       "      <td>253.560581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>137.758755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1539159711000</td>\n",
       "      <td>F</td>\n",
       "      <td>OH</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>paid</td>\n",
       "      <td>252.226546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>15.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1542993707000</td>\n",
       "      <td>F</td>\n",
       "      <td>OH-KY-IN</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>free</td>\n",
       "      <td>264.422171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>184.296004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1543415437000</td>\n",
       "      <td>F</td>\n",
       "      <td>FL</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>paid</td>\n",
       "      <td>249.587069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30.305638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1543515959000</td>\n",
       "      <td>M</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>paid</td>\n",
       "      <td>255.828061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_daily_sessions  avg_daily_items  avg_daily_session_duration  avg_daily_Settings  avg_daily_Add Friend  avg_daily_Thumbs Down  avg_daily_Downgrade  avg_daily_Submit Upgrade  avg_daily_Roll Advert  avg_daily_NextSong  avg_daily_Error  avg_daily_About  avg_daily_Upgrade  avg_daily_Add to Playlist  avg_daily_Home  avg_daily_Thumbs Up  avg_daily_Logout  avg_daily_Submit Downgrade  avg_daily_Save Settings  avg_daily_Help  lastIteraction gender   location  label  registeredDays last_level      length\n",
       "0  1.0                 27.825871        0.0                         1.0                 1.0                   1.0                    0.0                  0.0                       1.0                    1.0                 1.0              0.0              1.0                1.0                        1.0             1.0                  1.0               0.0                         1.0                      1.0             1542955611000   M       WA        0      72              free       253.560581\n",
       "1  1.0                 137.758755       0.0                         1.0                 1.0                   1.0                    1.0                  0.0                       0.0                    1.0                 0.0              1.0              0.0                1.0                        1.0             1.0                  1.0               0.0                         0.0                      0.0             1539159711000   F       OH        0      21              paid       252.226546\n",
       "2  1.0                 15.125000        0.0                         0.0                 1.0                   0.0                    0.0                  0.0                       1.0                    1.0                 0.0              0.0              0.0                0.0                        1.0             1.0                  0.0               0.0                         0.0                      0.0             1542993707000   F       OH-KY-IN  0      61              free       264.422171\n",
       "3  1.0                 184.296004       0.0                         1.0                 1.0                   1.0                    1.0                  1.0                       1.0                    1.0                 0.0              1.0              1.0                1.0                        1.0             1.0                  1.0               1.0                         1.0                      1.0             1543415437000   F       FL        0      82              paid       249.587069\n",
       "4  1.0                 30.305638        0.0                         1.0                 1.0                   1.0                    0.0                  1.0                       1.0                    1.0                 0.0              0.0              1.0                1.0                        1.0             1.0                  1.0               0.0                         0.0                      1.0             1543515959000   M       CA        0      77              paid       255.828061"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(features_df.take(5), columns=features_df.columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_df.select([count(when(isnull(c), c)).alias(c) for c in features_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- avg_daily_sessions: double (nullable = true)\n",
      " |-- avg_daily_items: double (nullable = true)\n",
      " |-- avg_daily_session_duration: double (nullable = true)\n",
      " |-- avg_daily_Settings: double (nullable = false)\n",
      " |-- avg_daily_Add Friend: double (nullable = false)\n",
      " |-- avg_daily_Thumbs Down: double (nullable = false)\n",
      " |-- avg_daily_Downgrade: double (nullable = false)\n",
      " |-- avg_daily_Submit Upgrade: double (nullable = false)\n",
      " |-- avg_daily_Roll Advert: double (nullable = false)\n",
      " |-- avg_daily_NextSong: double (nullable = false)\n",
      " |-- avg_daily_Error: double (nullable = false)\n",
      " |-- avg_daily_About: double (nullable = false)\n",
      " |-- avg_daily_Upgrade: double (nullable = false)\n",
      " |-- avg_daily_Add to Playlist: double (nullable = false)\n",
      " |-- avg_daily_Home: double (nullable = false)\n",
      " |-- avg_daily_Thumbs Up: double (nullable = false)\n",
      " |-- avg_daily_Logout: double (nullable = false)\n",
      " |-- avg_daily_Submit Downgrade: double (nullable = false)\n",
      " |-- avg_daily_Save Settings: double (nullable = false)\n",
      " |-- avg_daily_Help: double (nullable = false)\n",
      " |-- lastIteraction: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- registeredDays: integer (nullable = true)\n",
      " |-- last_level: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_features = features_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize=(20,15))\n",
    "#ax = fig.gca()\n",
    "#h = pd_features.hist(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = post_process_data(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test subsets\n",
    "train, test = model_data.randomSplit([0.8, 0.2], seed=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "logistic_regression:\n",
      "--------------------------------------------------\n",
      " \n",
      "                         Predict:\n",
      "                  Churn:     Not-Churn:\n",
      "        Churn:       7           3\n",
      "Real:\n",
      "        Not-Churn:   3           40\n",
      "\n",
      "Accuracy: 88.68%\n",
      "Precision: 70.00%\n",
      "Recall: 70.00%\n",
      "F1: 88.68%%\n",
      "AUC: 86.28%\n",
      "\n",
      "\n",
      "==================================================\n",
      "random_forest:\n",
      "--------------------------------------------------\n",
      " \n",
      "                         Predict:\n",
      "                  Churn:     Not-Churn:\n",
      "        Churn:       7           3\n",
      "Real:\n",
      "        Not-Churn:   2           41\n",
      "\n",
      "Accuracy: 90.57%\n",
      "Precision: 77.78%\n",
      "Recall: 70.00%\n",
      "F1: 90.37%%\n",
      "AUC: 94.19%\n",
      "\n",
      "\n",
      "==================================================\n",
      "gradient_boosting:\n",
      "--------------------------------------------------\n",
      " \n",
      "                         Predict:\n",
      "                  Churn:     Not-Churn:\n",
      "        Churn:       8           2\n",
      "Real:\n",
      "        Not-Churn:   8           35\n",
      "\n",
      "Accuracy: 81.13%\n",
      "Precision: 50.00%\n",
      "Recall: 80.00%\n",
      "F1: 82.60%%\n",
      "AUC: 90.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit various models and visualize their accuracies\n",
    "for model in ['logistic_regression', 'random_forest', 'gradient_boosting']:\n",
    "    fit_predict(train, test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score on the test set is 87.03%\n",
      "\n",
      "The areaUnderROC on the test set is 93.26%\n",
      "======FEATURES=========IMPORTANCE=========\n",
      "avg_daily_sessions : 0.0 \n",
      "\n",
      "avg_daily_items : 0.08744437069036085 \n",
      "\n",
      "avg_daily_session_duration : 0.0 \n",
      "\n",
      "avg_daily_Settings : 0.016216111818598958 \n",
      "\n",
      "avg_daily_Add Friend : 0.009976415593522693 \n",
      "\n",
      "avg_daily_Thumbs Down : 0.014341155220435609 \n",
      "\n",
      "avg_daily_Downgrade : 0.02593551061943221 \n",
      "\n",
      "avg_daily_Submit Upgrade : 0.015201497735093291 \n",
      "\n",
      "avg_daily_Roll Advert : 0.02609169877205655 \n",
      "\n",
      "avg_daily_NextSong : 0.0 \n",
      "\n",
      "avg_daily_Error : 0.02030323144682147 \n",
      "\n",
      "avg_daily_About : 0.01830046980787026 \n",
      "\n",
      "avg_daily_Upgrade : 0.013747358340235882 \n",
      "\n",
      "avg_daily_Add to Playlist : 0.004889348394876002 \n",
      "\n",
      "avg_daily_Home : 0.009852658058802892 \n",
      "\n",
      "avg_daily_Thumbs Up : 0.003800499176882964 \n",
      "\n",
      "avg_daily_Logout : 0.006566081731321905 \n",
      "\n",
      "avg_daily_Submit Downgrade : 0.018245264360672777 \n",
      "\n",
      "avg_daily_Save Settings : 0.023218183420334174 \n",
      "\n",
      "avg_daily_Help : 0.011735578521987537 \n",
      "\n",
      "lastIteraction : 0.4190742496348529 \n",
      "\n",
      "gender : 0.1698534558205737 \n",
      "\n",
      "location : 0.08520686083526749 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuned_rf = tuned_randomforest(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a9cc32c3b495c9bd44880fd1e9f356b83d64da460dfbbc2a86cea0304684740"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
